{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LetterDataset(Dataset):\n",
    "    def __init__(self, words, tokenizer, max_length=31):\n",
    "        self.words = words\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        letters = \" \".join(list(word))\n",
    "        encoded = self.tokenizer(\n",
    "            letters, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_length, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoded.input_ids.squeeze()\n",
    "        attention_mask = encoded.attention_mask.squeeze()\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        rand = torch.rand(input_ids.shape)\n",
    "        mask_arr = (rand < 0.15) * (input_ids != self.tokenizer.pad_token_id) * \\\n",
    "                   (input_ids != self.tokenizer.cls_token_id) * (input_ids != self.tokenizer.sep_token_id)\n",
    "        selection = torch.flatten(mask_arr.nonzero()).tolist()\n",
    "        input_ids[selection] = self.tokenizer.mask_token_id\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "def preprocess_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = [line.strip() for line in file.readlines()]\n",
    "    return words\n",
    "\n",
    "# Loading the words\n",
    "words = preprocess_words('words_250000_train.txt')\n",
    "\n",
    "# Initializing the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Creating the dataset\n",
    "dataset = LetterDataset(words, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cyclotrimethylenetrinitramine'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(words,key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] c y c l o t r [MASK] [MASK] e [MASK] h y l e n e t r i [MASK] i t [MASK] a m i n e [SEP]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[45222]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(words,key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45222\n"
     ]
    }
   ],
   "source": [
    "#getting the index of the maximun length word in the words list\n",
    "max_len_word_index = words.index(max(words,key=len))\n",
    "print(max_len_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/vishnu/AAT/GAR/GARvenv/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28415' max='28415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28415/28415 59:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.077556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.074300</td>\n",
       "      <td>0.070027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.066690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.062969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.061639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='711' max='711' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [711/711 00:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0617235042154789, 'eval_runtime': 55.6527, 'eval_samples_per_second': 816.852, 'eval_steps_per_second': 12.776, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Loading the pre-trained BERT model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Training \n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', \n",
    "    num_train_epochs=5,  \n",
    "    per_device_train_batch_size=32, \n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\",  \n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs', \n",
    "    logging_steps=500,  \n",
    "    load_best_model_at_end=True,  \n",
    ")\n",
    "\n",
    "# Initializing the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  # Providing the validation dataset\n",
    ")\n",
    "\n",
    "# Fine-tuning the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluating the model\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('hangman_model_finetuned_epoch5')\n",
    "model.save_pretrained('hangman_model_finetuned_epoch5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 0.2978498935699463), ('u', 0.21998746693134308), ('o', 0.21487948298454285), ('s', 0.16822588443756104), ('e', 0.14847537875175476), ('a', 0.14816780388355255), ('i', 0.10130421072244644), ('t', 0.07874676585197449), ('o', 0.07848888635635376), ('e', 0.07282354682683945), ('h', 0.06299653649330139), ('i', 0.059931814670562744), ('y', 0.057597316801548004), ('y', 0.03955281525850296), ('u', 0.03754061833024025), ('r', 0.024258708581328392), ('h', 0.012607723474502563), ('s', 0.006888314615935087), ('r', 0.00618574908003211), ('t', 0.006039596162736416)]\n",
      "Predicted letter: a\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "import random\n",
    "\n",
    "def predict_non_guessed_letter(masked_word, guessed_letters):\n",
    "    # Loading pre-trained BERT model and tokenizer\n",
    "    model_name = 'hangman_model_finetuned_epoch5'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenizing input\n",
    "    input_ids = tokenizer.encode(masked_word, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # List to hold the top 10 predictions for each masked position\n",
    "    top_predictions = []\n",
    "\n",
    "    # Get top 10 predictions for each masked token\n",
    "    masked_positions = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "    for masked_index in masked_positions:\n",
    "        probs = F.softmax(predictions[0, masked_index], dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, 10)\n",
    "        for prob, idx in zip(top_probs, top_indices):\n",
    "            token = tokenizer.decode([idx.item()]).strip()\n",
    "            top_predictions.append((token, prob.item()))\n",
    "\n",
    "    # Sort the predictions based on probability in descending order\n",
    "    top_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(top_predictions)\n",
    "    # Find the first letter not in guessed letters\n",
    "    for token, _ in top_predictions:\n",
    "        if token not in guessed_letters:\n",
    "            return token\n",
    "\n",
    "    #random guess if no letter found in top predictions that is not in guessed letters\n",
    "    all_letters = string.ascii_lowercase\n",
    "    remaining_letters = [letter for letter in all_letters if letter not in guessed_letters]\n",
    "    if remaining_letters:\n",
    "        return random.choice(remaining_letters)\n",
    "\n",
    "    return None  \n",
    "\n",
    "masked_word = \"c [MASK] t [MASK]\"\n",
    "guessed_letters = []\n",
    "predicted_letter = predict_non_guessed_letter(masked_word, guessed_letters)\n",
    "print(f\"Predicted letter: {predicted_letter}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
